{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erpanter/AdvancedTopicsInAI/blob/main/week2/convnets_with_small_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RZIgnFFEv5t"
      },
      "source": [
        "# Lab Exercise: Image Classification using Convolutional Neural Network\n",
        "\n",
        "In this practical, we will see how we can use a Convolutional Neural Network to classify cat and dog images.\n",
        "\n",
        "We will train the network using relatively little data (about 2000 images) which is a common real problem with a lot of deep learning projects where data is hard to come by. We have learnt in the lecture how we can solve the small data problem with some common techniques like data augmentation and transfer learning. We will examine how to use data augmentation in this lab and in the next lab, we will learn to use transfer learning.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQgbpqHw-vXy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydQUHIqs-vX6"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "The cats vs. dogs dataset is available at Kaggle.com as part of a computer vision\n",
        "competition in late 2013. You can download the [original dataset](\n",
        "https://www.kaggle.com/c/dogs-vs-cats/data) from Kaggle (you will need to create a Kaggle account if you don't already have one)\n",
        "\n",
        "The pictures are medium-resolution color JPEGs and are of various sizes and shapes that look like this:\n",
        "\n",
        "<img src='https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/resources/cats_vs_dogs_samples.png' height='300'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d76mWqf7-vX6"
      },
      "source": [
        "This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). For the purpose of demonstrating challenges of training with small data set and also to have an opportunity to see the effects of using data augmentation technique, we will use a smaller subset (3000 images) which you can download from [here](https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_subset.tar.gz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSQZGCRtReJY"
      },
      "source": [
        "In the codes below, we use the keras ``get_file()`` utility to download and unzip the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3JRhg0UBDRT1"
      },
      "outputs": [],
      "source": [
        "dataset_URL = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_subset.tar.gz'\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs_subset.tar.gz', origin=dataset_URL, extract=True, cache_dir='.')\n",
        "dataset_dir = os.path.join(os.path.dirname(path_to_zip), \"cats_and_dogs_subset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_rXZ-dlU-vYA",
        "outputId": "105b61de-bd58-4809-f2b2-30c64b09434b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './datasets/cats_and_dogs_subset/dogs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-068b446873b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdogs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dogs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcats_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cats\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total dog images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdogs_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total cat images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/cats_and_dogs_subset/dogs'"
          ]
        }
      ],
      "source": [
        "dogs_dir = os.path.join(dataset_dir, \"dogs\")\n",
        "cats_dir = os.path.join(dataset_dir, \"cats\")\n",
        "print('total dog images:', len(os.listdir(dogs_dir)))\n",
        "print('total cat images:', len(os.listdir(cats_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chQJRgxL-vYB"
      },
      "source": [
        "So we indeed have 3000 training images, 1500 each for cats and dogs. This is a balanced binary classification problem, which means that classification accuracy will be an appropriate measure of success."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMXLfrh6-vYB"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "Our convnet will be a stack of alternate `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
        "\n",
        "**Exercise 1**:\n",
        "\n",
        "Write the codes to implement the following:\n",
        "\n",
        "- Input layer should be of shape (128,128,3)\n",
        "- Rescaling layer to scale pixel values to between 0,1\n",
        "- The hidden layers consist of the following Conv2D/MaxPooling2D blocks:\n",
        "  - Block 1: Conv layer with 32 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - Block 2: Conv layer with 64 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - Block 3 and 4: Conv layer with 128 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - A Layer to convert 2D to 1D\n",
        "- A Dense Layer with 512 neurons\n",
        "- Output layer using Dense Layer\n",
        "\n",
        "Use RELU as activation functions for all hidden layers.\n",
        "\n",
        "What activation function should you use for the output layer?\n",
        "\n",
        "<br/>\n",
        "\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "def make_model():\n",
        "    \n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Input(shape=(128, 128, 3)))\n",
        "    model.add(keras.layers.Rescaling(scale=1./255))\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = make_model()\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "J-6zYJGn-vYB"
      },
      "outputs": [],
      "source": [
        "### TODO: Write the code to build the model and compile the model\n",
        "def make_model():\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Input(shape=(128, 128, 3)))\n",
        "    model.add(keras.layers.Rescaling(scale=1./255))\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = make_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EN4zoVi-vYC"
      },
      "source": [
        "Let's print the model summary to show the shape and paramater numbers for each layer. Your output should look something like this:\n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/resources/convnet_summary.png\" width=400/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6FJD8b_2-vYC",
        "outputId": "08e87561-9847-4d0d-b12d-d28064e4aa83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling_1 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,601,153\u001b[0m (9.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,601,153</span> (9.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,601,153\u001b[0m (9.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,601,153</span> (9.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqBfX0po-vYC"
      },
      "source": [
        "**Exercise 2**:\n",
        "\n",
        "Compile your model with the appropriate optimizer and loss function. We will use Adam with learning rate of 1e-4 and monitor the 'accuracy' metrics. What should we use for the loss function?\n",
        "\n",
        "Complete the code below.\n",
        "\n",
        "<br/>\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BOdPRQ8j-vYC"
      },
      "outputs": [],
      "source": [
        "### TODO: Complete the code below ####\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We2Khkc-vYC"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Image data should be formatted into appropriately pre-processed floating point tensors before being fed into our\n",
        "network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:\n",
        "\n",
        "* Read the picture files.\n",
        "* Decode the JPEG content to RGB grids of pixels.\n",
        "* Resize the image into same size (e.g, 128 x 128 in our case)\n",
        "* Convert these into floating point tensors.\n",
        "\n",
        "It may seem a bit daunting, but `tf.keras.preprocessing.image_dataset_from_directory()` (similar to `ImageDataGenerator` class) allows us to do all this rather painlessly. It also allows us to specify how to split the data into training and validation set.  We will use 80-20 split.\n",
        "\n",
        "`tf.keras.preprocessing.image_dataset_from_directory()` creates a dataset iterator using the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "usGtYvV8CK0J",
        "outputId": "9329e37c-c59f-4c1f-8b94-601d844aeb92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Could not find directory ./datasets/cats_and_dogs_subset",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-095eadb12c59>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_ds = keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory ./datasets/cats_and_dogs_subset"
          ]
        }
      ],
      "source": [
        "img_height, img_width = 128, 128\n",
        "batch_size = 32\n",
        "\n",
        "# resize all the images to the same size as expected by VGG model we downloaded above\n",
        "image_size = (img_height, img_width)\n",
        "\n",
        "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n",
        "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzFJ1ptz-vYD"
      },
      "source": [
        "We can use the `take(1)` to retrieve 1 batch of samples from the train dataset. As we specify batch size of 32,  it yields batches of 120 x 128 RGB images (shape `(32, 128, 128, 3)`) and binary\n",
        "labels (shape `(32,1)`). 32 is the number of samples in each batch (the batch size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tQJxIZGy-vYD",
        "outputId": "0c32db54-0b3f-433c-b3cc-2b4da557c795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_ds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-d0f79bcc1aeb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ],
      "source": [
        "for images, labels in train_ds.take(1):\n",
        "    print('images shape:', images.shape)\n",
        "    print('labels shape:', labels.shape)\n",
        "    print(tf.squeeze(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOx2NhAYQ0XF"
      },
      "source": [
        "How do we know what label is assigned to each of the class? We can use class_names of the dataset to show the class labels. The index position will be the actual numeric label mapped to the class names. In this case, cats=0, dogs=1. By default, `keras.preprocessing.image_dataset_from_directory` assign the labels based on alphanumerical order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fifOVDCQwva"
      },
      "outputs": [],
      "source": [
        "train_ds.class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tedmUKwPDHFe"
      },
      "source": [
        "## Visualization using Tensorboard\n",
        "\n",
        "Let's define a utility function to create a Tensorboard callback function that can be used by the model training later. We will also create a ModelCheckpoint callback tha allows us save the best checkpoint (in terms of validation accuracy) during the training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI2leTe5C2oR"
      },
      "outputs": [],
      "source": [
        "def create_tb_callback():\n",
        "\n",
        "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "    def get_run_logdir():    # use a new directory for each run\n",
        "\t    import time\n",
        "\t    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "\t    return os.path.join(root_logdir, run_id)\n",
        "\n",
        "    run_logdir = get_run_logdir()\n",
        "\n",
        "    tb_callback = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "    return tb_callback\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"bestcheckpoint.weights.h5\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4YMOgW8REAx"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Let's start the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwyFdTu8-vYD"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=30,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[create_tb_callback(), model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R9qafGC9U9e"
      },
      "source": [
        "Let's restore the best checkpoints to the model (remember the last checkpoint may not be the best model checkpoint). We will use the model to evaluate the our validation dataset, just to see what is our best validation accuracy achieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cEi0nA79o6e"
      },
      "outputs": [],
      "source": [
        "model.load_weights('bestcheckpoint.weights.h5')\n",
        "model.evaluate(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS2NA64l-vYE"
      },
      "source": [
        "Let's visualize our training and validation accuracy and loss using Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eje5UEye-vYE"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEI1hpgL67Ts"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tb_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOZTbSxL-vYE"
      },
      "source": [
        "These plots are characteristic of overfitting. Our training accuracy increases linearly over time, until it reaches nearly 100%, while our\n",
        "validation accuracy stalls at 75%. Our validation loss reaches its minimum after only five epochs then stalls, while the training loss\n",
        "keeps decreasing linearly until it reaches nearly 0.\n",
        "\n",
        "Because we only have relatively few training samples (2400), overfitting is going to be our number one concern. There are a\n",
        "number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We are now going to\n",
        "use one, specific to computer vision, and used almost universally when processing images with deep learning models: *data\n",
        "augmentation*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9YQpQ8-vYE"
      },
      "source": [
        "## Using data augmentation\n",
        "\n",
        "Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data.\n",
        "Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data\n",
        "augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number\n",
        "of random transformations that yield believable-looking images. This helps the model get exposed to more aspects of the data and generalize better.\n",
        "\n",
        "In the code below, we will check the tensorflow version and instantiate the correct layer depending on the version. We only one RandomRotation layer in the example below. The value 0.3 refers to the maximum rotation angle in both clock-wise and anti-clockwise direction. You can find out more info from the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbAfiBdu-vYE"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.RandomRotation(0.3)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chGB_tp0OmVI"
      },
      "source": [
        "\n",
        "To see the effects of data augmentation, let us apply our data_augmentation layer to a sample image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGJAgFv7OojQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images, _ = next(train_ds.take(1).as_numpy_iterator())\n",
        "sample_image = images[0]/255.\n",
        "plt.imshow(sample_image)\n",
        "sample_image = tf.expand_dims(sample_image, 0)\n",
        "print(sample_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gtBzOgkP3ng"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "for i in range(19):\n",
        "    augmented_image = data_augmentation(sample_image)\n",
        "    ax = plt.subplot(5, 4, i + 1)\n",
        "    plt.imshow(augmented_image[0])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qiDkZc-vYF"
      },
      "source": [
        "**Exercise 3:**\n",
        "\n",
        "Modify `data_augmention` above to add in [Random Flipping](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomFlip) and [Random Zoom](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomZoom). Choose the appropriate values for the flipping and cropping heights/widths.\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.RandomRotation(0.3),\n",
        "        keras.layers.RandomFlip(mode=\"horizontal\"),\n",
        "        keras.layers.RandomZoom(0.2)\n",
        "    ]\n",
        ")\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxLPiFmw-vYF"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.RandomRotation(0.3),\n",
        "        keras.layers.RandomFlip(mode=\"horizontal\"),\n",
        "        keras.layers.RandomZoom(0.2)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I6r5H9B-vYF"
      },
      "source": [
        "**Exercise 4:**\n",
        "\n",
        "Modify `make_model()` to apply data augmention layers you have created earlier. Where should you place your augmentation layer?\n",
        "\n",
        "To further fight overfitting, we will also add a Dropout layer to our model, right before the densely-connected classifier.\n",
        "\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "def make_model():\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Input(shape=(128, 128, 3)))\n",
        "    model.add(data_augmentation)\n",
        "    model.add(keras.layers.Rescaling(scale=1./255))\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "    model.add(keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = make_model()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfk0cqS2YHYj"
      },
      "outputs": [],
      "source": [
        "def make_model():\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        "\n",
        "    model.add(keras.layers.Input(shape=(128, 128, 3)))\n",
        "    model.add(data_augmentation)\n",
        "    model.add(keras.layers.Rescaling(scale=1./255))\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "    model.add(keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = make_model()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUsyk60w8Ow5"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5fmnlwSG6vq"
      },
      "source": [
        "Let's train our network using data augmentation and dropout. We also need to train for more epochs, so that our network has better chance of seeing all the original images (since now we cannot guarantee that for each epoch, our original image is chosen at least once, instead, the ImageDataGenerator may choose randomly transformed image instead)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1eWVr6WKuo9"
      },
      "outputs": [],
      "source": [
        "### Note: the training will take quite a while. We have previously trained the model for 100-epochs.\n",
        "### You can download the checkpoints by uncommenting the following and skip the next cell \"mode.fit()\"\n",
        "\n",
        "# !wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/checkpoints/week3-1-100epochs.zip\n",
        "# !unzip week3-1-100epochs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv2sEKTrYl81"
      },
      "outputs": [],
      "source": [
        "## Comment out this if you just want to use the pretrained weights\n",
        "model.fit(train_ds, validation_data=val_ds,\n",
        "          epochs=100,\n",
        "          callbacks=[create_tb_callback(), model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI1D6tyc-HPT"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"bestcheckpoint.weights.h5\")\n",
        "model.evaluate(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRZY8r1O-vYG"
      },
      "source": [
        "Let's visualize our training using Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LZDuZaW-vYG"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tb_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOLypNoy-vYG"
      },
      "source": [
        "Thanks to data augmentation and dropout, we are no longer overfitting: the training curves are more closely tracking the validation\n",
        "curves. We are now able to reach an accuracy of 80%, slightly better than previously.\n",
        "\n",
        "However, it would be very difficult to improve the model any further even with data augmentation. The augmented images are still heavily correlated, since they come from a small number of original images -- we cannot produce new information, we can only remix existing information. As next step to improve our accuracy on this problem, we will have to leverage transfer learning using pre-trained model, which will be the focus of the lesson."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "convnets-with-small-datasets.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}